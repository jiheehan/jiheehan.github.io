---
title: "Masked Autoencoders Are Scalable Vision Learners"
date: 2023-02-12 17:36:28
author_profile: true
categories: DeepLearning
---
# [Paper Review] Masked Autoencoders Are Scalable Vision Learners

 2021년 11월에 Facebook AI Research (FAIR) 에서 발표했고 Kaiming He 가 제 1저자로 참여한 논문이다.

 논문에서 제안하는 아이디어는 NLP 분야에서 주로 사용되던 masked autoencoder를 computer vision 분야에 적용하여 self-supervised 방식으로 이미지 context를 학습 (pre-training)하고 다른 목적의 task (classification, object detection, segmentation 등)로 확장하는 방식이다.

 autoencoder, self-supervised learning에 대한 배경 지식이 있으면 읽기 더 편했을 것 같다. 읽을 당시 나에게는 생소한 분야이다 보니 내 주관을 최대한 섞지 않고 저자가 논문에 쓴 글을 거의 그대로 해석하면서 한 줄 한 줄 꼼꼼히 읽다보니 리뷰라기 보다는 번역이 되었다. 이 분야에서 다소 trivial한 것들에 대해서 자세하게 명시하지 않거나 reference 논문만 기재해놓은 부분들이 있어 완전하게 이해하지 못하고 넘어간 것들이 많은데 이런 점들 때문에 실험 내용에 대한 부분을 이해하기 어려웠다.

# Introduction

 지난 몇 년간 딥러닝 성능이 폭발적으로 향상했고 그 배경에는 딥러닝 네트워크 크기 증가와 더 커진 네트워크를 학습시킬 하드웨어 성능 향상이 있다. 네트워크 크기가 증가하면서 데이터 백만 장 정도는 쉽게 오버피팅이 되어버리고 더 많은 데이터가 요구된다. 

 NLP에서는 이러한 다량의 데이터가 요구되는 문제를 self-supervised learning 기법(GPT, BERT)을 통해서 해결하고 있다. 이에 해당되는 SOTA 기법으로는 autoregressive language 모델인 GPT가 있고, masekd autoencoder를 이용한 BERT가 있다. 그 중 BERT가 본 논문이 적용한 기법 (masked autoencoder)과 가장 유사한 아이디어로, 개념적으로는 매우 간단한 접근이다: 데이터의 일부를 제거 (masking)하여 제거된 데이터를 예측하도록 학습시킨다. 이런 방법을 통해 현재는 1000억개가 넘는 파라미터를 가진 NLP 모델을 generalization이 가능하도록 학습시킬 수 있다.

 masked autoencoder는 좀 더 general한 형태의 denoising autoencoder (데이터에 noise를 가한 뒤 noise가 제거된 데이터를 예측하도록 학습시킨 autoencoder) 중 한 종류로 볼 수 있는데 이것은 computer vision 분야에도 자연스럽게 적용가능하다. 그럼에도 NLP분야에서 BERT의 성공과는 달리 vision 분야에서는 비교적 낮은 성능을 가지는데, 그 이유를 찾아보고자 한다.

#### "masked autoencoder가 vision과 language 모델에 적용할 때 다른 것이 무엇일까"

 ##### 	1. architecture가 다르다

​		vision에서는 convolutional network가 dominant하게 쓰이는데, 이 convolution 구조에 masked autoencoder의 mask token이나 positional embeddings를 접목시키기 어렵다. 그러나 이러한 architecture적인 gap은 Vision Transformers (ViT) 에서 해결되었다.

##### 	2. Information density

​		language와 vision은 information density가 다르다. language는 사람이 생성한 신호로 단어 하나하나가 높은 수준의 의미를 가지는 반면, 이미지는 natural signals로 픽셀 하나하나가 그다지 높은 의미를 가지지 않으면서 spatial redundancy (불필요한 중복)를 가진다. 이미지에서 일부 픽셀을 missing한 경우 "높은 수준의 이해없이" 주변 픽셀을 이용하여 복원할 수 있다. 이것은 네트워크가 학습할 내용에 대한 의미레벨 (semantic level)에 크게 영향을 미치는데, language의 경우 missing된 단어를 예측하도록 학습할 경우 네트워크는 높은 수준의 지능을 학습하게 되고 image의 픽셀을 예측하도록 학습할 경우는 단순하고 낮은 수준의 이해 관계를 학습하게 된다.   

​	본 논문에서는 vision 용도의 maksed autoencoder가 이런 차이를 극복하고 더 쓸모있는 feature를 학습 할 수 있도록 간단한 전략을 사용한다

##### 		'masking a very high portion of random patches'

​	이 방법은 redundacy를 없애고 이미지에서도 전반적인 (holistic) 이해를 요구하는 self-supervising task를 수행하게 한다.

#####		3. autoencoder의 decoder (latent representation을 input 차원으로 되돌리는 부분)가 text와 image에서 다른 역할을 한다.

​		vision에서 decoder는 pixel을 복원하므로 출력값은 다른 일반적인 인식 task들에 비해 의미레벨 (semantic lavel)이 낮다. 반대로 language에서 decoder는 missing words를 예측하므로 rich한 의미레벨을 포함하고 있다.  BERT에서 decoder는 단순 MLP 구조로 trivial하지만 images에서는 decoder 디자인이 학습된 latent representation의 의미레벨을 결정하는데 중요한 역할을 한다는 점을 알아냈다.



 이런 분석들을 통해 본 논문에서는 간단하고 효과적이면서 다른 task를 수행하기 위한 네트워크로 확장가능한 구조의 MAE (Masked AutoEncoder)를 제안한다. MAE는 input image에서 랜덤하게 패치단위로 masking하고 masking된 패치를 픽셀레벨로 복원한다. 비대칭적 (asymmetric) encoder-decoder 구조로, encoder는 mask tokens 없이 보이는 패치의 부분에서만 연산하고 decoder는 가벼운 구조로 mask tokens를 포함하여 latent representation으로부터 input을 복원한다. (Figure 1)

 비대칭적 autoencoder구조에서 mask token을 작은 decoder로 이동시킨 것은 계산량을 많이 줄였다. 이 구조에서 매우 높은 masking ratio (e.g., 75%)는 win-win 시나리오를 달성한다. encoder단에서 일부 작은 패치부분에서만 (e.g., 25%) 연산하면서 정확도를 최적화시키는 것은 pretraining 시간을 3배 또는 그 이상을 줄이고 메모리 소비량도 줄여 MAE를 더 큰 모델로 확장시킬 수 있게 한다.

  제안하는 MAE는 generalize가 잘 되는 high-capacity 모델을 학습시킬수 있다. MAE pretraining 모델을 이용하여 ImageNet-1K에 대해 향상된 generalize 성능을 보이는 ViT-Large/-Huge (데이터가 많이 필요한)모델을 학습시킬 수 있다. vanilla ViT-Huge 모델에서 ImageNet-1K로 fine-tuning했을 때 87.8% 정확도를 가진다. 이것은 이전의 ImageNet-1K data만을 이용한 다른 방법들보다 성능이 뛰어나다. 또한 object detection, instance segmentation, semantic segmentation으로 transfer learning에 대해서 평가를 진행했는데, supervised pretraining 기법들을 사용한 것보다 좋은 결과를 보였고 모델을 scaling-up하면서 의미있는 결과를 발견했다.

# Related Work

#### Masked language modeling

 masked language 모델인 BERT와 autoregressive 모델인 GPT는 NLP분야에서 매우 성공적인 방법들이고 둘 다 input sequence에서 일부를 가리고 missing content를 예측하도록 학습시킨다. 이 방법들은 pre-trained representation이 다양한 downstream task에도 일반화 (generalize)가 잘 되고 scale이 성공적으로 되는 것을 많은 실험을 통해 입증한다.  

#### Autoencoding

 autoencoding은 representation을 학습시키는 기본적인 방법이다. autoencoder는 input을 latent representation으로 매핑하는 encoder와 input을 복원하는 decoder를 가진다. 예를 들어, PCA 와 k-means 는 autoencoder이다. Denoising autoencoder (DAE)는 autoencoder의 일종으로 input 신호에 noise를 주고 (corrupt) noise가 없는 original signal (uncorrupted signal)을 복원하도록 한다. 픽셀을 masking하거나 color 채널을 없애는 등의 corruption을 주는 것은 일반화된 DAE로 볼 수 있다. 제안하는 MAE도  denoising autoencoder의 한 종류이긴 하지만 여러 방면에서 클래식한 DAE와 차이가 있다. (클래식한 DAE와 MAE의 차이에는 asymmetric 한 구조에서 차이가 있다.)

#### Masked image encoding

 masked image encoding은 masking에 의해 corrupt된 이미지로부터 representation을 학습한다. DAE의 noise로 masking을 취했던 연구도 있었고, Context Encoder는 넓은 missing 영역을 convolutional network를 통해 inpaint한다. NLP의 성공에 힘입어 최근 masked image encoding 방법들은 Transformer를 기반으로 한다. iGPT는 픽셀의 sequence에서 연산하고 unknown 픽셀을 예측한다. ViT는 self-supervised 학습을 위해 masked patch를 예측한다. 가장 최근에는 BEiT가 discrete tokens를 예측하는 방법이 제안되었다. 

#### Self-supervised learning

컴퓨터 비전분야에서 self-supervised learning approach들이 관심받고 있는데 pretraining 할 때 다른 pre-text task에 집중한다. 최근에는 image의 similarity와 dissimilarity를 모델링 하는 contrastive learning이 유행이다. 그러나 contrastive learning과 관련 기법들은 data augmentation에 의존적이다. autoencoding은 개념적으로 다른 방향을 추구하며 그것은 앞으로 보일 내용에서 다른 결과를 보인다.

# Approach

 제안하는 masked autoencoder (MAE)는 심플한 autoencoding 방식으로 original signal의 일부를 가지고 전체를 복원한다. 다른 autoencoder들과 같이 observed signal (가려지지 않은 부분)을 latent representation으로 매핑하는 encoder와 latent representation으로부터 original signal을 복원하는 decoder를 가진다.

 고전적인 autoencoder들과 다르게, 본 논문에서는 encoder단에서 mask token 없이 observed signal에 대해서만 연산하고 뒷단에 lightweight (가벼운) decoder를 통해 latent representation과 mask token들을 이용하여 전체 signal 을 복원하는 비대칭적 구조를 택했다.

(Figure 1)

#### Masking

 ViT처럼 이미지 영역을 중복된 없이 균등한 patch단위로 나눴다. 그 다음으로 patch들 중 일부를 샘플링하고 나머지 부분을 masking 한다. (전체 영역에 대해서 uniform distribution하게 radom 샘플링)

#### MAE encoder

ViT와 유사하지만 visible (unmasked) patches에 대해서만 적용을 한다는 점에서 차이가 있다. ViT처럼 positional embeddings을 추가한 linear projection을 통해 patch들을 encoder에 입력하고 연속적인 transformer block 연산을 수행한다. 그러나 MAE의 encoder에서는 일부 작은 부분 (e.g., 25%)에 대해서만 연산을 하고, mask token들이 사용되지 않는다. 이것을 통해 연산량과 메모리를 줄여 매우 큰 encoder를 학습시킬 수 있다.

#### MAE decoder

MAE decoder의 입력은 token들의 full set으로 (i) encoding된 visible patch들과 (ii) mask token들로 구성되어 있다. 각각의 mask token은 공유되고 학습되는 벡터값으로 예측되어야할 missing patch들의 존재를 가리킨다.  이 token full set의 모든 token에 positional embeddings을 더한다.; 이것이 없으면 mask token들은 이미지에서 그들의 위치에 대한 정보가 없을 것이다. decoder는 다른 transformer block의 연속이다.

 MAE decoder는 image 복원 task를 수행하는 pre-training을 진행하는 동안만 사용된다. (인식 task를 위한 image representation을 만드는 데에는 encoder만 사용됨) 그러므로 decoder 구조는 encoder와 독립된 방식으로 유연하게 설계될 수 있다. encoder에 비해 좁고 얕은 (아주 작은) decoder로 실험을 진행했다. 실험한 default decoder는 encoder 대비 token당 10% 미만의 연산량을 가진다. 이런 비대칭적 구조로 token들의 full set은 pre-training시간을 줄여주는 가벼운 decoder로만 처리가 된다.

#### Reconstruction target

MAE는 각각의 masking된 patch들의 pixel 값들을 예측함으로써 input을 복원한다. 각각의 decoder의 output은 patch를 나타내는 pixel 값에 대한 벡터이다. decoder의 마지막 레어어는 출력 채널이 patch 안의 픽셀의 개수와 동일한 linear projection이다. decoder의 출력은 복원된 이미지의 형태로 reshape 된다. loss function은 픽셀 space에서 복원된 이미지와 original이미지 사이의 mean square error (MSE) 이고 loss값은 masked patch들에 대해서만 계산한다.

 reconstruction target을 normalized pixel값으로 했을 때 representation의 quality를 향상시켰다. 모든 patch 안의 pixel들에 대한 mean과 standard deviation을 계산하여 patch의 normalize하는 데 사용했다.

#### Simple implementation

 MAE pre-training은 별도 연산없이 효율적으로 가능하다. 우선, (positional embedding들을 추가한 linear projection을 통해) 모든 input patch에 대하여 token을 생성한다. 다음으로 token 리스트들을 radom하게 shuffle시킨 뒤, masking ratio에 따라 일부를 제거한다. 이 프로세스는 encoder의 입력을 위한 token들의 작은 부분집합을 생성하는 것으로, replacement 없는 patch sampling과 동일하다. encoding 후에는 encoded된 patch 리스트에 mask token을 덧붙이고 (append),  모든 token이 각각의 target에 맞도록 그 full list의 순서를 되돌린다. (unshuffle, inverting the random shuffle operation; 이것을 위해서는 encoder 단에서 shuffle 시킬 때 각각 흩어진 위치정보를 담고 있어야 할 것 같다.) decoder는 이 full list (positional embedding을 추가한) 에 대해 적용된다. 참고로 shuffle과 inshuffle에 대한 overhead는 무시할 만 하다.

# ImageNet Experiment

#### Main Properties

#### Comparisons with Previous Results

#### Partial Fine-tuning

# Transfer Learning Experiments

# Discussion and Conclusion
